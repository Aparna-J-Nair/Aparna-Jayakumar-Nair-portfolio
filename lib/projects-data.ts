export interface Project {
  id: string
  title: string
  organization: string
  date: string
  category: "Professional" | "Academic"
  description: string[]
  technologies: string[]
  impact?: string
  github?: string // Added GitHub link field
}

export const projects: Project[] = [

  {
    id: "protego-safety-chatbot",
    title: "Protego Safety Assistant Chatbot",
    organization: "Cummins Inc.",
    date: "Jul 2025",
    category: "Professional",
    description: [
      "üß† Project Overview: The Protego Safety Assistant is an intelligent Retrieval-Augmented Generation (RAG) chatbot designed to assist users in accessing critical safety information through natural text and voice interactions.",
      "The system integrates OpenAI GPT models, Whisper, LangChain, and FAISS to deliver fast, accurate, and context-aware responses ‚Äî enabling seamless access to safety guidelines, procedures, and compliance information within industrial environments.",
      "This project demonstrates how a multimodal AI pipeline can be deployed to handle both speech and text queries, perform semantic retrieval from structured datasets, and generate human-like explanations tailored to workplace safety scenarios.",
      "‚öôÔ∏è Methodology - Step 1: Knowledge Base Creation",
      "Curated and preprocessed structured safety and compliance datasets from internal repositories.",
      "Used OpenAI's Embeddings API to convert safety documents into semantic vector representations that capture contextual meaning beyond keyword matching.",
      "Indexed all embeddings in FAISS (Facebook AI Similarity Search) for efficient high-dimensional vector lookup.",
      "Implemented LangChain's VectorStore abstraction to manage ingestion, indexing, and retrieval pipelines atop FAISS.",
      "‚öôÔ∏è Methodology - Step 2: Voice and Text Input Processing",
      "Supported dual-mode interaction ‚Äî users could either type queries or speak through voice input.",
      "Integrated OpenAI Whisper for real-time speech-to-text transcription of user audio queries.",
      "Standardized all incoming inputs as text, ensuring consistent downstream processing regardless of modality.",
      "‚öôÔ∏è Methodology - Step 3: Retrieval-Augmented Generation Pipeline",
      "Converted the user's query (typed or transcribed) into embeddings using OpenAI's Embedding model.",
      "Performed a similarity search against the FAISS index to retrieve the most semantically relevant safety information.",
      "Leveraged LangChain's RetrievalQA Chain to dynamically construct prompts that combine user questions with contextual snippets from retrieved documents.",
      "Passed the enriched prompt to OpenAI's GPT-4o model to generate a coherent, natural, and safety-compliant response.",
      "‚öôÔ∏è Methodology - Step 4: Response Generation & Delivery",
      "GPT-4o produced a detailed, human-like answer grounded in retrieved context ‚Äî minimizing hallucination and ensuring factual accuracy.",
      "Responses were returned to the chatbot interface in text form, with the potential for text-to-speech playback in future expansions.",
      "LangChain served as the middleware, orchestrating interactions between Whisper, the embedding model, FAISS, and GPT.",
      "üìä Results: Enabled context-aware voice and text retrieval of safety documentation with high semantic accuracy.",
      "Delivered sub-second query response times through optimized FAISS vector search.",
      "Achieved a 95%+ precision rate in retrieving relevant safety responses.",
      "Significantly improved accessibility and efficiency in navigating safety procedures at Cummins.",
    ],
    technologies: [
      "Python",
      "Flask",
      "OpenAI GPT-4o",
      "OpenAI Whisper",
      "LangChain",
      "FAISS",
      "OpenAI Embeddings",
      "Vector Databases",
      "Retrieval-Augmented Generation (RAG)",
      "Natural Language Processing",
      "Speech-to-Text",
    ],
    impact: "Achieved 95%+ precision in safety information retrieval with sub-second response times",
  },
  {
    id: "solar-plant-analysis",
    title: "Data-Based Analysis of Solar Plant",
    organization: "New Mexico State University",
    date: "Jan 2024 - Present",
    category: "Professional",
    description: [
      "Designed and implemented a Modbus TCP data acquisition pipeline to collect real-time solar power plant data for organized long-term monitoring.",
      "Developed a real-time data processing system that decodes raw Modbus register values (float32/float64, configurable endianness) into engineering units ensuring robustness with parallel processing and fault tolerance.",
      "Conducted data analysis for anomaly detection in solar plant operations using statistical and machine learning techniques.",
      "Developing real-time interactive dashboards for data visualization to enhance monitoring and decision-making processes.",
    ],
    technologies: ["Python", "Modbus TCP", "Machine Learning", "Data Visualization", "Real-time Processing"],
  },
  {
    id: "agrivoltaics-shading-analysis",
    title: "Designing and Shading Analysis for AgriVoltaics",
    organization: "New Mexico State University",
    date: "Jan 2024 - Present",
    category: "Professional",
    description: [
      "Designed solar panels for 3 different locations, shading analysis of different types of solar trackers is conducted.",
      "Proposed algorithm and mathematical formula for the shading analysis of single-axis solar panels.",
      "It is specifically analyzed in collaboration with the agriculture department funded by DOE.",
    ],
    technologies: ["Python", "Solar Energy", "Algorithm Design", "Mathematical Modeling"],
    impact: "Funded by Department of Energy (DOE)",
  },
  {
    id: "smart-compose-text-completion",
    title: "Smart Compose for Text Completion",
    organization: "LTI Mindtree",
    date: "Aug 2023",
    category: "Professional",
    description: [
      "üß† Project Overview: This project aimed to replicate and extend the functionality of Google's Smart Compose feature by building a real-time predictive text generation system powered by Generative AI models.",
      "The goal was to create an intelligent text-assist API capable of predicting and streaming the next word or phrase dynamically as the user types, enabling faster, context-aware writing experiences in web applications.",
      "The system combined OpenAI and GPT-based language models with a Flask RESTful backend and a live HTML interface, delivering low-latency predictions and high contextual accuracy.",
      "‚öôÔ∏è Methodology - Step 1: Architecture & Design",
      "Designed a Flask-based RESTful API for real-time text prediction.",
      "Integrated OpenAI GPT models and other LLMs for generative completion.",
      "The architecture followed a streaming-based design where the model outputs tokens progressively as the user types, simulating real-time autocomplete behavior.",
      "Implemented asynchronous request handling to minimize response delay and support concurrent users.",
      "‚öôÔ∏è Methodology - Step 2: Model Integration",
      "Leveraged OpenAI GPT models for contextual prediction of next words and phrases.",
      "Fine-tuned models on domain-specific text to improve sentence coherence and stylistic accuracy.",
      "Experimented with prompt-engineering strategies for better control of tone, formality, and sentence continuation quality.",
      "Incorporated probabilistic beam search and top-k sampling to optimize diversity and relevance of completions.",
      "‚öôÔ∏è Methodology - Step 3: Frontend & User Interaction",
      "Built an interactive HTML/JavaScript frontend that captures user keystrokes and dynamically requests predictions from the API.",
      "Displayed model-suggested completions inline (in light gray), similar to Gmail's Smart Compose experience.",
      "Allowed users to accept predictions using tab/enter keys, ensuring a seamless, natural text-writing experience.",
      "‚öôÔ∏è Methodology - Step 4: Optimization & Deployment",
      "Applied response caching and token streaming to reduce latency and maintain sub-second response times.",
      "Deployed the system locally and later containerized it for scalable deployment across web environments.",
      "Monitored model inference speed and optimized token streaming for real-time completion.",
      "üìä Results: Delivered a Smart Compose-like experience with <1.2s average response latency.",
      "Improved text prediction accuracy and contextual relevance through GPT model fine-tuning.",
      "Seamlessly integrated into web-based editors and chat interfaces via REST API.",
      "Demonstrated adaptability for enterprise applications such as email drafting, customer support, and report summarization.",
    ],
    technologies: [
      "Python",
      "Flask",
      "RESTful API",
      "OpenAI GPT Models",
      "HTML",
      "JavaScript",
      "Generative AI",
      "Natural Language Processing",
      "WebSocket Streaming",
      "Prompt Engineering",
      "Token Streaming",
    ],
  },
  
  {
    id: "multimodal-data-extraction",
    title: "Multimodal Data Extraction & SDLC Correlation",
    organization: "LTI Mindtree",
    date: "May - Jul 2023",
    category: "Professional",
    description: [
      "üß© Project Overview: This project focused on building a multimodal data extraction and correlation pipeline that automatically processes image, PDF, audio, and video content to generate actionable insights linked to Software Development Life Cycle (SDLC) artifacts.",
      "The system integrates Optical Character Recognition (OCR), speech-to-text transcription, Large Language Models (LLMs), and vector search frameworks to enable intelligent querying of organizational documentation and media assets.",
      "The solution was deployed as a Flask-based API, allowing users to query organizational data in natural language and retrieve relevant visual, textual, or audio insights correlated to SDLC phases such as requirement analysis, testing, and deployment.",
      "‚öôÔ∏è Methodology - Step 1: Multimodal Data Ingestion",
      "Image & Document Extraction: Implemented OCR pipelines using OpenCV, Tesseract, and PyPDF2 to extract text from scanned documents, screenshots, PDFs, and PowerPoint slides. Cleaned and preprocessed extracted text using regex-based normalization and document-structure parsing.",
      "Video Frame Analysis: Processed videos frame-by-frame using OpenCV to detect and extract embedded textual or visual information. Applied OCR on keyframes and correlated timestamps to specific SDLC milestones.",
      "Audio Transcription: Employed OpenAI's Whisper model for high-accuracy transcription of meetings, voice notes, and recorded presentations. Generated synchronized timestamp-based text segments for downstream analysis.",
      "‚öôÔ∏è Methodology - Step 2: Data Processing & Semantic Storage",
      "Extracted text, transcripts, and frame-level content were embedded into vector representations using sentence transformers.",
      "Indexed multimodal embeddings in Elasticsearch and FAISS for hybrid search (semantic + keyword-based retrieval).",
      "Designed metadata schemas tagging each document or media segment with SDLC attributes (e.g., 'Design,' 'Development,' 'Testing,' 'Deployment').",
      "‚öôÔ∏è Methodology - Step 3: Query Correlation Engine",
      "Developed a Flask API that accepts natural-language queries such as: 'Show all test cases discussed in project meeting recordings' or 'Retrieve requirement documents related to UI module.'",
      "The query is embedded and searched against the multimodal vector store, retrieving correlated text, images, or transcript excerpts.",
      "Implemented a correlation algorithm that cross-references retrieved data with SDLC metadata to identify contextual relevance.",
      "‚öôÔ∏è Methodology - Step 4: Insight Generation",
      "Leveraged LLMs to summarize retrieved segments and generate concise, human-readable insights.",
      "Enabled automatic creation of SDLC documentation drafts (e.g., test summaries, requirement overviews) from the extracted information.",
      "Supported output in multiple formats ‚Äî tabular data, summarized text, or document snippets.",
      "üìä Results: Achieved 95% OCR accuracy on varied document types and languages.",
      "Enabled end-to-end multimodal correlation across image, text, and audio data sources.",
      "Reduced manual documentation and data-retrieval time by over 70%.",
      "Delivered a scalable backend API that supports semantic organizational knowledge retrieval in real-time.",
    ],
    technologies: [
      "Python",
      "Flask",
      "OpenCV",
      "Tesseract OCR",
      "PyPDF2",
      "OpenAI Whisper",
      "Elasticsearch",
      "FAISS",
      "Sentence Transformers",
      "LLMs",
      "Data Correlation",
      "Vector Search",
      "NLP",
    ],
    impact: "95% accuracy in OCR-based extraction, 70% reduction in manual documentation time",
  },
  {
    id: "rule-based-nlg",
    title: "Rule-Based Natural Language Generation",
    organization: "LTI Mindtree",
    date: "Mar 2023",
    category: "Professional",
    description: [
      "üß† Project Overview: This project focused on developing an interactive data analytics dashboard capable of transforming user queries into dynamic visualizations and natural language insights.",
      "The system combined rule-based query parsing with GPT-based text generation to enable users to interact conversationally with tabular data ‚Äî asking analytical questions and receiving both tabular and textual explanations of the results.",
      "The core idea was to bridge structured data (tables) and natural language by integrating rule-driven logic, neural language models, and table-question answering (TQA) capabilities for explainable analytics.",
      "‚öôÔ∏è Methodology - Step 1: Data Input & Query Interface",
      "Designed a web-based dashboard that allows users to upload tabular datasets (CSV, Excel).",
      "Implemented a query input module where users can type questions such as: 'What is the average sales by region?' or 'Which product category had the highest growth last quarter?'",
      "Parsed user queries into structured commands using a rule-based parser designed to identify: Target column(s) (e.g., 'sales,' 'profit'), Aggregation type (e.g., 'average,' 'sum,' 'max'), and Grouping or filtering attributes (e.g., 'by region,' 'in 2022').",
      "‚öôÔ∏è Methodology - Step 2: Rule-Based Query Processing",
      "Created a set of custom logic rules mapping natural language keywords to dataframe operations in Python (via Pandas).",
      "Example mappings: 'average' ‚Üí df[column].mean(), 'by region' ‚Üí df.groupby('region')",
      "This rule-based layer ensured reliable interpretation of analytical queries even before integrating GPT models.",
      "‚öôÔ∏è Methodology - Step 3: Table Question Answering (TQA) using TAPAS",
      "Integrated Google's TAPAS model (Table Parser Sequence-to-Sequence) to perform neural table question answering.",
      "TAPAS enabled direct reasoning over structured tables without converting data into plain text.",
      "Compared TAPAS responses with rule-based outputs to validate consistency and expand the system's reasoning capability.",
      "‚öôÔ∏è Methodology - Step 4: Dynamic Visualization & Text Generation",
      "Used Matplotlib and Plotly to generate dynamic charts and tables based on query outputs (e.g., bar charts, pie charts, trends).",
      "Integrated GPT-based NLG (Natural Language Generation) to generate contextual explanations of visual results, e.g.: 'The East region reported the highest average sales of $52,000, outperforming other regions by 14%.'",
      "Combined rule-based insights with GPT's interpretive summaries, resulting in both precision and natural fluency.",
      "‚öôÔ∏è Methodology - Step 5: Dashboard Integration",
      "Built a unified dashboard that returns: The filtered output table, An automatically generated visualization, and A GPT-generated textual summary describing the findings.",
      "üìä Results: Enabled users to query complex datasets in natural language without requiring SQL or programming knowledge.",
      "Achieved high interpretability by fusing structured outputs (tables, charts) with human-readable GPT explanations.",
      "Successfully validated TAPAS as a complementary model for advanced table-based reasoning.",
      "Demonstrated a seamless data-to-text workflow integrating NLG and visual analytics.",
    ],
    technologies: [
      "Python",
      "Pandas",
      "Matplotlib",
      "Plotly",
      "Flask",
      "GPT Models",
      "TAPAS",
      "Hugging Face",
      "Natural Language Processing",
      "Rule-Based Systems",
      "Data Visualization",
      "NLG",
      "Table Question Answering",
    ],
    impact:
      "Enabled non-technical users to query complex datasets in natural language, achieving seamless data-to-text workflow with high interpretability",
  },
  {
    id: "topic-modeling-gpt",
    title: "Topic Modeling Using GPT",
    organization: "LTI Mindtree",
    date: "Dec 2022",
    category: "Professional",
    description: [
      "üß† Project Overview: This project focused on building an AI-driven topic modeling and interpretation pipeline that automatically extracts, clusters, and labels key topics from large volumes of unstructured text.",
      "The workflow integrated BERTopic for unsupervised keyword extraction with GPT-Neo for semantic topic naming through prompt engineering. Additionally, several Hugging Face transformer models were evaluated to enhance clustering quality and topic interpretability.",
      "The final solution was deployed as a Flask-based API, capable of ingesting raw text and returning structured, interpretable topic insights in real time ‚Äî significantly improving automation in thematic analysis, document classification, and knowledge management systems.",
      "‚öôÔ∏è Methodology - Step 1: Data Preprocessing & Embedding Generation",
      "Collected diverse unstructured datasets (e.g., text logs, reviews, and corporate reports).",
      "Applied Sentence-BERT embeddings for dense semantic representation of each document.",
      "Performed dimensionality reduction using UMAP and clustering using HDBSCAN to identify semantically coherent topic groups.",
      "‚öôÔ∏è Methodology - Step 2: Keyword Extraction using BERTopic",
      "Implemented BERTopic, which combines transformer embeddings and class-based TF-IDF (c-TF-IDF) for generating representative topic keywords.",
      "Tuned hyperparameters for optimal topic cohesion and minimized redundancy.",
      "Validated extracted keywords via intra-cluster similarity analysis and coherence scoring.",
      "‚öôÔ∏è Methodology - Step 3: Topic Label Generation via GPT-Neo and Hugging Face Models",
      "Integrated GPT-Neo (1.3B parameters) to generate concise and contextually rich topic names using prompt engineering.",
      "Designed dynamic prompt templates feeding cluster keywords into GPT-Neo for interpretive summarization.",
      "Experimented with alternative Hugging Face models ‚Äî including GPT-2, DistilGPT-2, Flan-T5, and BART ‚Äî to compare fluency, contextual accuracy, and domain adaptability.",
      "GPT-Neo produced the most semantically consistent and human-like topic names, while Flan-T5 showed strength in factual summarization.",
      "‚öôÔ∏è Methodology - Step 4: API Development & Deployment",
      "Built a Flask REST API integrating the full pipeline: Text preprocessing ‚Üí Clustering ‚Üí Keyword extraction ‚Üí Topic naming.",
      "Deployed endpoints returning structured JSON output containing: Topic ID, Representative Keywords, LLM-generated Topic Label, Coherence and Confidence Scores.",
      "üìä Results: Achieved high topic coherence and semantic relevance across multiple text domains.",
      "Automated the labeling process, reducing manual curation effort by over 80%.",
      "Comparative testing across Hugging Face models provided insights into trade-offs between model interpretability and computational efficiency.",
      "GPT-Neo and Flan-T5 emerged as the best-performing models for balanced interpretability and topic precision.",
    ],
    technologies: [
      "Python",
      "Flask",
      "BERTopic",
      "GPT-Neo",
      "Hugging Face Transformers",
      "GPT-2",
      "DistilGPT-2",
      "Flan-T5",
      "BART",
      "Sentence-BERT",
      "UMAP",
      "HDBSCAN",
      "Prompt Engineering",
      "REST APIs",
      "Topic Modeling",
      "Clustering",
    ],
    impact: "Reduced manual topic curation effort by over 80% through automated AI-driven pipeline",
  },
  {
    id: "gait-analysis-cybersecurity",
    title: "Gait Analysis for Cybersecurity",
    organization: "University of Wyoming",
    date: "Jun 2022",
    category: "Professional",
    description: [
      "üß† Project Overview: This research investigates cybersecurity vulnerabilities in gait-based biometric authentication by developing a novel video gait spoofing attack model. The study demonstrates that motion sensor-based authentication systems, often considered highly secure, can be compromised through the correlation between gait video and motion sensor data collected from wearable devices.",
      "The project was conducted under the joint guidance of Dr. Premjith B (Amrita Vishwa Vidyapeetham) and Dr. Diksha Shukla (University of Wyoming) and represents one of the first practical demonstrations of adversarial attacks on motion sensor-based gait authentication systems.",
      "‚öôÔ∏è Methodology - Step 1: Data Acquisition",
      "Collected synchronized motion sensor (accelerometer + gyroscope) and video gait data from 15 participants.",
      "Motion data captured via smartwatches at 50 Hz; video data recorded using smartphones mounted at waist level.",
      "Each participant walked back and forth for 2-3 minutes to generate consistent gait cycles.",
      "‚öôÔ∏è Methodology - Step 2: Motion Sensor Authentication Model",
      "Extracted statistical and temporal features (e.g., autocorrelation, FFT coefficients, entropy, wavelet transforms) using tsfresh library.",
      "Selected the top 29 discriminative features via Correlation-Based Feature Selection (CFS) in Weka.",
      "Built motion sensor-based authentication models using SVM, Random Forest (RF), Linear Discriminant Analysis (LDA), and Neural Network (NNet) classifiers.",
      "Implemented data balancing using SMOTE to address class imbalance between legitimate and imposter samples.",
      "Achieved >90% authentication accuracy and a mean Equal Error Rate (EER) of 5.6% using SVM.",
      "‚öôÔ∏è Methodology - Step 3: Correlation & Attack Modeling",
      "Applied OpenPose for video-based human pose estimation to extract 2D keypoints of gait motion.",
      "Computed Pearson correlation between video features and motion sensor features to identify 29 highly correlated feature pairs.",
      "Developed an attack model exploiting these correlations ‚Äî an adversary with access to a target's gait video could infer correlated motion features and bypass motion sensor authentication.",
      "Attack models were tested using SVM, RF, LDA, and NNet classifiers on a balanced binary dataset (legitimate vs. imposter).",
      "üß© Experimental Findings",
      "The proposed attack model successfully deceived motion sensor-based authentication systems.",
      "Observed a significant increase in mean EER across all classifiers, confirming reduced model robustness post-attack.",
      "Demonstrated that attackers leveraging gait video correlations can gain unauthorized access without direct sensor data.",
      "Visualized per-user EER degradation and classifier-wise vulnerability through comparative analysis plots.",
      "üõ°Ô∏è Discussion & Implications",
      "Highlighted a critical cybersecurity risk in current motion sensor-based biometric systems.",
      "Suggested multi-modal defense strategies, including randomization of sensor sampling and differential privacy mechanisms.",
      "Proposed the use of multi-sensor fusion and behavioral anomaly detection to enhance resistance to spoofing.",
    ],
    technologies: [
      "Machine Learning",
      "Computer Vision",
      "OpenPose",
      "tsfresh",
      "Weka",
      "SMOTE",
      "Dimensionality Reduction",
      "Clustering",
      "MATLAB",
      "Python",
      "SVM",
      "Random Forest",
      "Neural Networks",
    ],
    impact: "Published in Springer Conference Proceedings",
  },
  {
    id: "question-answering-bert",
    title: "Question Answering Using BERT",
    organization: "Amrita Vishwa Vidyapeetham",
    date: "Dec 2022",
    category: "Academic",
    description: [
      "Implemented a question answering system using BERT transformer model.",
      "Fine-tuned pre-trained BERT model on domain-specific datasets for improved accuracy.",
      "Developed an end-to-end pipeline for processing questions and extracting relevant answers from text.",
    ],
    technologies: ["BERT", "Transformers", "NLP", "Python", "Hugging Face"],
  },
  {
    id: "speech-recognition-hubert",
    title: "Automatic Speech Recognition Using HuBERT",
    organization: "Amrita Vishwa Vidyapeetham",
    date: "Aug 2022",
    category: "Academic",
    description: [
      "üéØ Project Overview: Developed an automatic speech recognition system combining HuBERT (Hidden unit BERT) and Conformer architectures to achieve state-of-the-art performance on speech recognition tasks.",
      "üß† HuBERT (Hidden unit BERT): Implemented a new approach for learning self-supervised speech representations to model rich lexical and non-lexical information in audio. The HuBERT model either matches or improves upon the state-of-the-art wav2vec 2.0 performance on the Librispeech (960h) dataset.",
      "Achieved up to 19% and 13% relative WER (Word Error Rate) reduction on the more challenging dev-other and test-other evaluation subsets, demonstrating significant improvements in speech recognition accuracy.",
      "The simplicity and stability of HuBERT helps natural language processing and speech researchers to more broadly adopt learned discrete representations in their work.",
      "The quality of HuBERT's learned representations facilitates easy deployment to many different downstream speech applications, making it versatile for various use cases.",
      "üîÑ Conformer: Convolution-augmented Transformer for Speech Recognition: Implemented Conformer architecture which combines convolution neural networks and transformers to capture both local and global dependencies of an audio sequence.",
      "On the widely used LibriSpeech benchmark, the model achieves WER of 2.1%/4.3% without using a language model and 1.9%/3.9% with an external language model on test-clean/test-other subsets.",
      "üìä Dataset - LibriSpeech: Utilized LibriSpeech corpus containing approximately 1000 hours of 16kHz read English speech. The whole corpus is divided into train, test, and validation sets. This project specifically used the test-clean subset for evaluation.",
      "The combination of HuBERT's self-supervised learning and Conformer's hybrid architecture demonstrates the power of modern deep learning approaches in achieving human-level speech recognition accuracy.",
    ],
    technologies: [
      "HuBERT",
      "Conformer",
      "Speech Recognition",
      "Audio Processing",
      "Deep Learning",
      "Python",
      "Transformers",
      "CNN",
      "Self-Supervised Learning",
      "LibriSpeech",
    ],
    github: "https://github.com/Aparna-J-Nair/Automatic-Speech-Recognition_Term-Project-using-Hubert-and-Conformer",
    impact:
      "Achieved state-of-the-art WER of 1.9%/3.9% on LibriSpeech benchmark with 19% and 13% relative WER reduction on challenging subsets",
  },
  {
    id: "underwater-acoustic-analysis",
    title: "Underwater Acoustic Signal Extraction and Analysis",
    organization: "Amrita Vishwa Vidyapeetham",
    date: "Sept 2022",
    category: "Academic",
    description: [
      "üéØ Project Overview: This research-driven project explores underwater acoustic signal generation, denoising, and direction of arrival (DOA) estimation using both signal processing and deep learning techniques.",
      "The work was divided into two integrated phases ‚Äî the first applying machine learning‚Äìbased decomposition methods for signal cleaning, and the second leveraging deep neural networks (ResNet-50) for acoustic direction estimation.",
      "By combining advanced time‚Äìfrequency decomposition algorithms with deep residual learning, the project demonstrates how hybrid data-driven models can significantly enhance underwater communication, marine target tracking, and acoustic sensing accuracy in noisy oceanic environments.",
      "üåä === **PHASE I: MACHINE LEARNING APPROACH** === üåä",
      "üéØ Acoustic Signal Extraction - Objective: To extract weak underwater acoustic signals from heavy environmental noise using three advanced denoising techniques ‚Äî Empirical Mode Decomposition (EMD), Variational Mode Decomposition (VMD), and Synchro-Squeezing Transform (SST) ‚Äî and to evaluate their comparative performance.",
      "üîß Signal Generation: Simulated underwater acoustic signals (frequency range 1995‚Äì2005 Hz) in MATLAB. Added synthetic ship noise, reverberation, and Gaussian noise to replicate realistic marine conditions. Varied Signal-to-Noise Ratios (SNR) between ‚Äì10 dB and +10 dB.",
      "üß™ Algorithms Applied: EMD decomposed the signal into Intrinsic Mode Functions (IMFs) but exhibited mode-mixing under low SNR. VMD used adaptive decomposition approach yielding tightly centered frequency modes with superior denoising capability. SST employed time‚Äìfrequency reassignment method offering sharper spectral localization for reconstruction.",
      "üìè Evaluation Metrics: Compared algorithms using SNR, SNR gain, and Root Mean Square Error (RMSE) under varying noise conditions.",
      "üìä Phase I Results: VMD achieved the best results with the highest SNR gain and lowest RMSE across all test cases. Successfully reconstructed clean target waveforms from highly degraded noisy inputs. Demonstrated that VMD > EMD > SST in terms of denoising performance and stability.",
      "ü§ñ === **PHASE II: DEEP LEARNING APPROACH** === ü§ñ",
      "üéØ Direction of Arrival (DOA) Estimation - Objective: To estimate the direction of arrival of simulated underwater acoustic signals using a deep residual convolutional neural network (ResNet-50).",
      "üì¶ Dataset Creation: Generated 658 synthetic samples representing acoustic signals arriving from angles ‚Äì90¬∞ to +90¬∞ in 30¬∞ steps. Converted each into a 201 √ó 201 √ó 2 covariance matrix encoding magnitude and phase.",
      "üèóÔ∏è Model Architecture: Implemented ResNet-50, enhanced with a flattening layer and dense layers for classification into seven angular classes. Used SoftMax activation, Adam optimizer (LR = 0.0001), and categorical cross-entropy loss.",
      "‚öôÔ∏è Training Setup: Split data into 80% training / 20% testing. Conducted comparative trials using VGG16 vs. ResNet-50, observing improved convergence and generalization in the latter.",
      "üìä Phase II Results: ResNet-50 achieved Training Accuracy: ‚âà 55%, Validation Accuracy: ‚âà 45%, Testing Accuracy: ‚âà 38%. Increasing the number of array elements improved DOA classification reliability. The model successfully learned unique covariance-based spatial patterns corresponding to different source directions.",
    ],
    technologies: [
      "MATLAB",
      "Python",
      "Signal Processing Toolbox",
      "Deep Learning Toolbox",
      "ResNet-50",
      "NumPy",
      "Matplotlib",
      "EMD",
      "VMD",
      "SST",
      "DOA Estimation",
      "Acoustic Signal Processing",
    ],
    github: "https://github.com/Aparna-J-Nair/Underwater-Acoustic-Analysis",
    impact:
      "Demonstrated hybrid approach combining ML-based signal denoising with deep learning for DOA estimation, advancing underwater acoustic sensing capabilities",
  },
  {
    id: "depression-detection-tweets",
    title: "Detecting Depression in Tweets using Bayes Theorem",
    organization: "Amrita Vishwa Vidyapeetham",
    date: "May 2022",
    category: "Academic",
    description: [
      "Developed a sentiment analysis system to detect signs of depression in Twitter posts, addressing the growing concern of mental illness and early death risk due to depression.",
      "Depression creates suicidal thoughts causing serious impairments in daily life. This project aims to overcome impediments such as social stigma associated with mental disorders, lack of trained healthcare professionals, and ignorance of the signs of depression.",
      "Extracted features from combined effects of emotional, momentary, and grammatical processes for detecting and processing depressive data retrieved from social media platforms.",
      "Implemented comprehensive pre-processing pipeline including removal of stop words, stemming, and tokenization to prepare data for analysis.",
      "Converted the dataset into frequency tables using TF-IDF (Term Frequency-Inverse Document Frequency) to reflect how important a word is to a document in the collection.",
      "Applied Naive Bayes classifier to calculate depressed and non-depressed words per comment/post, enabling accurate classification of tweets.",
      "Successfully classified tweets into depressed and non-depressed categories based on curated word lists and statistical analysis.",
      "Methodology: Data retrieval from Twitter API ‚Üí Pre-processing (stop words removal, stemming, tokenization) ‚Üí TF-IDF conversion ‚Üí Naive Bayesian classification ‚Üí Final classification into depressed/non-depressed categories.",
    ],
    technologies: [
      "Python",
      "Naive Bayes",
      "NLP",
      "Text Classification",
      "Sentiment Analysis",
      "TF-IDF",
      "Twitter API",
      "Scikit-learn",
    ],
    github: "https://github.com/Aparna-J-Nair/Detecting-depression-in-Tweets-using-Bayes-Theorem-Naive-Bayes-",
    impact:
      "Addresses critical mental health detection challenges by identifying depression signs in social media posts",
  },
  {
    id: "movie-classification-dnn",
    title: "Classifying Movies using Deep Neural Network",
    organization: "Amrita Vishwa Vidyapeetham",
    date: "May 2021",
    category: "Academic",
    description: [
      "üé• Project Overview: This project focuses on automatically classifying movies into genres using a Deep Neural Network (DNN). Movies often belong to multiple genres ‚Äî for example, 'Action-Comedy' or 'Drama-Romance' ‚Äî so the model was designed as a multi-label classification system capable of recognizing more than one genre per movie.",
      "The model leverages deep learning techniques implemented in MATLAB's Neural Network Toolbox, showcasing how DNNs can extract and learn high-level semantic relationships between movie attributes (such as synopsis, keywords, cast, and other metadata) and their corresponding genres.",
      "Data Preparation: Compiled a dataset of movies containing textual and categorical features such as title, description, keywords, and user tags. Converted raw categorical and text-based information into numeric feature vectors suitable for neural network training (e.g., using one-hot encoding and word frequency mapping). Normalized input data to ensure balanced feature scaling across all movie attributes.",
      "Model Design: Constructed a Deep Neural Network (DNN) using MATLAB's patternnet and trainNetwork functions. Designed the architecture with multiple fully connected hidden layers, ReLU activation functions for nonlinearity, dropout layers to prevent overfitting, and sigmoid output layer for multi-label prediction (each genre treated as an independent binary classification task).",
      "Training & Optimization: Split the dataset into training and validation subsets using MATLAB's cvpartition. Trained the model with stochastic gradient descent (SGD) and adaptive learning rates. Tuned hyperparameters such as number of neurons per layer, learning rate, and regularization strength for improved performance. Used confusion matrices, ROC curves, and precision-recall metrics to assess classification quality across genres.",
      "Multi-Label Classification: Implemented a sigmoid cross-entropy loss function, enabling the network to predict multiple active genres per movie. Evaluated predictions using Hamming Loss and F1-score for multi-label evaluation.",
      "üìä Results: Achieved high classification accuracy across major genres such as Action, Comedy, Romance, Thriller, and Drama. The network successfully captured overlapping features, allowing it to classify multi-genre films accurately. Visualization of learning curves and confusion matrices provided insights into model performance and genre relationships.",
    ],
    technologies: [
      "MATLAB",
      "Deep Learning Toolbox",
      "Neural Network Toolbox",
      "Signal Processing Toolbox",
      "Data Preprocessing",
      "Statistical Analysis",
      "Multi-Label Classification",
      "Deep Neural Networks",
    ],
    impact:
      "Successfully implemented multi-label classification system capable of accurately predicting multiple genres per movie",
  },
  {
    id: "speech-emotion-recognition",
    title: "CNN based Speech Emotion Recognition",
    organization: "Amrita Vishwa Vidyapeetham",
    date: "Dec 2020",
    category: "Academic",
    description: [
      "üß† Project Overview: This project focuses on emotion recognition from human speech using deep learning. The goal was to train a Convolutional Neural Network (CNN) that automatically identifies emotional states ‚Äî such as happy, sad, angry, calm, or neutral ‚Äî directly from raw audio recordings.",
      "By combining signal processing with deep feature learning, this system demonstrates how AI can understand and respond to human emotions ‚Äî a key step toward emotion-aware interfaces, virtual assistants, and affective computing systems.",
      "üéµ Methodology - Audio Data Handling: The project uses the RAVDESS dataset (Ryerson Audio-Visual Database of Emotional Speech and Song), containing thousands of labeled .wav files. Each file encodes the speaker's emotion in its filename (e.g., 03-01-03-01-02-05.wav ‚Üí happy). The script systematically traverses dataset folders, loads each file, and maps file codes to human emotion labels.",
      "Feature Extraction: Extracted Mel-Frequency Cepstral Coefficients (MFCCs) ‚Äî a key representation of human auditory perception. Averaged across time frames to obtain a 40-dimensional feature vector per audio file. Additionally visualized waveforms and MFCC spectrograms using librosa.display for exploratory analysis.",
      "Data Preprocessing: Encoded categorical emotion labels numerically using LabelEncoder and converted to one-hot vectors. Standardized all features using StandardScaler. Split the dataset into training and testing sets (80/20). Expanded dimensions to make the data compatible with the CNN input shape (40, 1).",
      "CNN Model Architecture: The CNN was implemented using TensorFlow/Keras with four 1D convolutional layers (16, 32, 64, 128 filters), MaxPooling1D after each convolutional block to reduce feature dimensionality, Dropout layers to prevent overfitting, Flatten layer to transform convolutional features, and fully connected layers (128 ‚Üí 64 ‚Üí 8 neurons) with ReLU and final softmax activation. The model was compiled with Adam optimizer and categorical cross-entropy loss, and trained for 25 epochs with batch size 64.",
      "Evaluation & Visualization: Achieved high accuracy on both training and testing sets, validating the network's ability to generalize to unseen voices. Plotted accuracy vs. epoch curves for training and validation. Generated a confusion matrix and heatmap to analyze misclassifications across emotional categories.",
      "üìä Results: Model Accuracy demonstrated strong classification accuracy across all eight emotions (Neutral, calm, happy, sad, angry, fearful, disgust, and surprised). Visualization through heatmaps revealed that closely related emotions (e.g., calm vs. neutral) were most challenging, reflecting real-world variability in emotional tone.",
    ],
    technologies: [
      "Python",
      "TensorFlow",
      "Keras",
      "NumPy",
      "Pandas",
      "Librosa",
      "Scikit-learn",
      "Matplotlib",
      "Seaborn",
      "CNN",
      "MFCC",
    ],
    github: "https://github.com/Aparna-J-Nair/Convolutional-Neural-Network-Based-Speech-Emotion-Recognition",
    impact:
      "Demonstrated AI's ability to understand and respond to human emotions - a key step toward emotion-aware interfaces and affective computing systems",
  },
  {
    id: "transfer-learning",
    title: "Transfer Learning: Leveraging Pre-trained Knowledge for New Tasks",
    organization: "Amrita Vishwa Vidyapeetham",
    date: "May 2022",
    category: "Academic",
    description: [
      "Overview: Transfer learning enables the application of knowledge acquired from one task (source task) to improve learning in another related task (target task). This approach is particularly useful when training data for the target task is limited.",
      "Implemented transfer learning using pre-trained VGG model, originally designed for classifying images into 1000 categories, and adapted it for different applications such as identifying abnormalities in medical images.",
      "Feature Extraction Approach: Used a pre-trained model as a feature extractor for new tasks. The network's existing layers remained unchanged, while a new classification layer was added on top. Only the additional classifier was trained, preserving the original model's learned features.",
      "Fine-Tuning Approach: Made modifications to the pre-trained model's parameters to better align with the target task. Added a new layer and adjusted selected layers from the pre-trained model using a lower learning rate to prevent drastic changes. Lower layers (capturing general patterns) were frozen, while higher layers (focused on task-specific features) were fine-tuned.",
      "Case 1 - Small Target Dataset, Similar Tasks: Applied feature extraction to leverage pre-trained features without extensive retraining, preventing overfitting on small datasets.",
      "Case 2 - Small Target Dataset, Different Tasks: Extracted general features from lower layers of the pre-trained model while removing or replacing upper, task-specific layers for hybrid approach.",
      "Case 3 - Large Target Dataset, Similar Tasks: Used pre-trained model to initialize the network and fine-tuned a few layers to accelerate learning and improve performance.",
      "Case 4 - Large Target Dataset, Different Tasks: Fine-tuned a substantial portion of the network to ensure effective adaptation to new data distribution and task requirements.",
      "Demonstrated how transfer learning enhances learning efficiency by utilizing pre-trained models, reducing training time and computational costs across various domains including computer vision, natural language processing, and reinforcement learning.",
    ],
    technologies: [
      "Transfer Learning",
      "Deep Learning",
      "VGG",
      "CNN",
      "Feature Extraction",
      "Fine-Tuning",
      "Python",
      "TensorFlow",
      "Keras",
      "Computer Vision",
    ],
    github: "https://github.com/Aparna-J-Nair/Transfer-Learning-Leveraging-Pre-trained-Knowledge-for-New-Tasks",
    impact:
      "Demonstrated powerful approach for enhancing learning efficiency by utilizing pre-trained models, reducing training time and computational costs",
  },
]

export function getProjectById(id: string): Project | undefined {
  return projects.find((project) => project.id === id)
}

export function getProjectsByCategory(category: "Professional" | "Academic"): Project[] {
  return projects.filter((project) => project.category === category)
}
